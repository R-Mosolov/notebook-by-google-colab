{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parse_articles_content.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1lW3MEOMyIU8h50TgrzYs-FNyfwV6MThD",
      "authorship_tag": "ABX9TyPoULQx0mQTeCt4vSiDdYqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-Mosolov/sociology-scientometric-analysis/blob/main/parse_articles_content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b4Z1fT5o-7W"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import requests\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFje6aWOpNSu"
      },
      "source": [
        "'''\n",
        "Show an article content\n",
        "'''\n",
        "fp = urllib.request.urlopen(\"https://cyberleninka.ru/article/n/metodologicheskie-problemy-sravnitelnogo-analiza-paradigm-tehnicheskoy-realnosti\")\n",
        "mybytes = fp.read()\n",
        "\n",
        "html_doc = mybytes.decode(\"utf8\")\n",
        "fp.close()\n",
        "\n",
        "# Get HTML structure to parse\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "article_html = soup\n",
        "\n",
        "article_html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUq40pXhdMOd"
      },
      "source": [
        "article_html.body.find('i', attrs={ 'itemprop': 'keywords' }).find_all('span')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeFhAufNqNx5"
      },
      "source": [
        "'''\n",
        "Define parse targets with usefull information\n",
        "'''\n",
        "views = article_html.body.find('div', attrs={ 'class': 'views' }).get_text()\n",
        "downloads = article_html.body.find('div', attrs={ 'class': 'downloads' }).get_text()\n",
        "journal_title = article_html.body.find('div', attrs={ 'class': 'half' }).span.a.get_text()\n",
        "journal_link = article_html.body.find('div', attrs={ 'class': 'half' }).span.a['href']\n",
        "abstract = article_html.body.find('div', attrs={ 'class': 'abstract' }).p.get_text()\n",
        "\n",
        "# Integrate all key words\n",
        "key_words_html = article_html.body.find('i', attrs={ 'itemprop': 'keywords' }).find_all('span')\n",
        "key_words = []\n",
        "for key_word in key_words_html:\n",
        "  key_words.append(key_word.get_text().lower())\n",
        "\n",
        "# Integrate all article paragraphs\n",
        "article_text_html = article_html.body.find_all('p')\n",
        "article_text = []\n",
        "for article_paragraph in article_text_html:\n",
        "  article_text.append(article_paragraph.get_text())\n",
        "\n",
        "print('--- Содержимое статьи ---')\n",
        "print('Просмотров:', views)\n",
        "print('Скачиваний:', downloads)\n",
        "print('Название журнала:', journal_title)\n",
        "print('Гиперссылка журнала:', journal_link)\n",
        "print('Ключевые слова:', key_words)\n",
        "print('Аннотация:', abstract)\n",
        "print('Текст статьи:', article_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yJH1RbnRj7N"
      },
      "source": [
        "'''\n",
        "Show information about the dataset with articles links\n",
        "'''\n",
        "df = pd.read_csv('/content/drive/MyDrive/Science/Datasets/cyberleninka-sociology-articles/cyberleninka-sociology-articles__1-7-4_gen-with-article-links.csv')\n",
        "\n",
        "print('Датасет с ссылками на статьи содержит:', len(df), 'строк')\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AkXqqW5YCVC"
      },
      "source": [
        "'''\n",
        "Put all articles links into an one array\n",
        "'''\n",
        "articles_links = df['article_link']\n",
        "only_articles_links = []\n",
        "\n",
        "for article_link in articles_links:\n",
        "  only_articles_links.append(article_link)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsZgqE47UzW6"
      },
      "source": [
        "'''\n",
        "The combinated algorithm to parse articles informations\n",
        "with realization of sessions rotation \n",
        "'''\n",
        "# Create data structure\n",
        "class Article:\n",
        "  def __init__(\n",
        "    self, article_number, article_link, article_views, article_downloads,\n",
        "    journal_title, journal_link, key_words, abstract, article_text\n",
        "  ):\n",
        "    self.article_number = article_number\n",
        "    self.article_link = article_link\n",
        "    self.article_views = article_views\n",
        "    self.article_downloads = article_downloads\n",
        "    self.journal_title = journal_title\n",
        "    self.journal_link = journal_link\n",
        "    self.key_words = key_words\n",
        "    self.abstract = abstract\n",
        "    self.article_text = article_text\n",
        "\n",
        "# Set condition to clear a buffer stopping parser's work\n",
        "is_column_name = True\n",
        "proxies_quantity = 6\n",
        "result_file_name = 'cyberleninka-sociology-articles__2-1-2_articles-content.csv'\n",
        "EMPTY = 'EMPTY'\n",
        "ERROR = 'ERROR'\n",
        "df = pd.read_csv('/content/drive/MyDrive/Science/Datasets/cyberleninka-sociology-articles/cyberleninka-sociology-articles__1-7-4_gen-with-article-links.csv')\n",
        "\n",
        "# Put all articles links into an one array\n",
        "articles_links = df['article_link']\n",
        "only_articles_links = []\n",
        "\n",
        "for article_link in articles_links:\n",
        "  only_articles_links.append(article_link)\n",
        "\n",
        "# TODO: Delete this block after finishing the parser work\n",
        "resized_df = slice(2646, len(only_articles_links))\n",
        "only_articles_links = only_articles_links[resized_df]\n",
        "\n",
        "# Initialize proxies for parsing\n",
        "proxies = pd.read_excel('/content/drive/MyDrive/Science/Datasets/proxies/proxies.xlsx')\n",
        "\n",
        "proxies_with_ports = [\n",
        "  proxies['proxy_with_port'][0],\n",
        "  proxies['proxy_with_port'][1],\n",
        "  proxies['proxy_with_port'][2],\n",
        "  proxies['proxy_with_port'][3],\n",
        "  proxies['proxy_with_port'][4],\n",
        "  proxies['proxy_with_port'][5]\n",
        "]\n",
        "proxy_login = proxies['login'][0]\n",
        "proxy_password = proxies['password'][0]\n",
        "\n",
        "ip_addresses = [\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[0],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[1],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[2],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[3],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[4],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[5]\n",
        "]\n",
        "  \n",
        "articles = []\n",
        "counter = 0\n",
        "\n",
        "# Remove old file\n",
        "try:\n",
        "  os.remove('/content/' + result_file_name)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "for article_link in only_articles_links:\n",
        "  # Increase an article index\n",
        "  counter += 1\n",
        "    \n",
        "  try:\n",
        "\n",
        "    # Create an user's session\n",
        "    url = \"https://cyberleninka.ru\" + str(article_link)\n",
        "\n",
        "    user_session = requests.Session()\n",
        "    proxies = {\n",
        "      'http': ip_addresses[counter % proxies_quantity],\n",
        "      'https': ip_addresses[counter % proxies_quantity]\n",
        "    }\n",
        "\n",
        "    response = user_session.get(url, proxies=proxies)\n",
        "    html_doc = response.content\n",
        "\n",
        "    # Get HTML structure to parse\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "    article_html = soup\n",
        "\n",
        "    # Get an article views\n",
        "    try:\n",
        "      article_views = article_html.body.find('div', attrs={ 'class': 'views' }).get_text()\n",
        "      if article_views:\n",
        "        article_views = article_views\n",
        "      else:\n",
        "        article_views = EMPTY\n",
        "    except:\n",
        "      article_views = ERROR\n",
        "\n",
        "    # Get an article downloads\n",
        "    try:\n",
        "      article_downloads = article_html.body.find('div', attrs={ 'class': 'downloads' }).get_text()\n",
        "      if article_downloads:\n",
        "        article_downloads = article_downloads\n",
        "      else:\n",
        "        article_downloads = EMPTY\n",
        "    except:\n",
        "      article_downloads = ERROR\n",
        "\n",
        "    # Get a journal title\n",
        "    try:\n",
        "      journal_title = article_html.body.find('div', attrs={ 'class': 'half' }).span.a.get_text()\n",
        "      if journal_title:\n",
        "        journal_title = journal_title\n",
        "      else:\n",
        "        journal_title = EMPTY\n",
        "    except:\n",
        "      journal_title = ERROR\n",
        "\n",
        "    # Get a journal link\n",
        "    try:\n",
        "      journal_link = article_html.body.find('div', attrs={ 'class': 'half' }).span.a['href']\n",
        "      if journal_link:\n",
        "        journal_link = journal_link\n",
        "      else:\n",
        "        journal_link = EMPTY\n",
        "    except:\n",
        "      journal_link = ERROR\n",
        "\n",
        "    # Get an article key words\n",
        "    try:\n",
        "      key_words_html = article_html.body.find('i', attrs={ 'itemprop': 'keywords' }).find_all('span')\n",
        "      key_words = []\n",
        "      for key_word in key_words_html:\n",
        "        key_words.append(key_word.get_text().lower())\n",
        "\n",
        "      if key_words:\n",
        "        key_words = key_words\n",
        "      else:\n",
        "        key_words = EMPTY\n",
        "    except:\n",
        "      key_words = ERROR\n",
        "\n",
        "    # Get an article abstract\n",
        "    try:\n",
        "      abstract = article_html.body.find('div', attrs={ 'class': 'abstract' }).p.get_text()\n",
        "      if abstract:\n",
        "        abstract = abstract\n",
        "      else:\n",
        "        abstract = EMPTY\n",
        "    except:\n",
        "      abstract = ERROR\n",
        "\n",
        "    # Get an article text\n",
        "    try:\n",
        "      article_text_html = article_html.body.find_all('p')\n",
        "      article_text = []\n",
        "      for article_paragraph in article_text_html:\n",
        "        article_text.append(article_paragraph.get_text())\n",
        "\n",
        "      if article_text:\n",
        "        article_text = article_text\n",
        "      else:\n",
        "        article_text = EMPTY\n",
        "    except:\n",
        "      article_text = ERROR\n",
        "  \n",
        "  except:\n",
        "    article_link = ERROR\n",
        "    article_views = ERROR\n",
        "    article_downloads = ERROR\n",
        "    journal_title = ERROR\n",
        "    journal_link = ERROR\n",
        "    key_words = ERROR\n",
        "    abstract = ERROR\n",
        "    article_text = ERROR\n",
        "\n",
        "  # Put data about an article to main array\n",
        "  articles.append(Article(\n",
        "    counter, article_link, article_views, article_downloads,\n",
        "    journal_title, journal_link, key_words, abstract, article_text\n",
        "  ))\n",
        "\n",
        "  # Save parsed data as an Excel file\n",
        "  with open(result_file_name, 'a') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    if is_column_name == True:\n",
        "      writer.writerow([\n",
        "        'article_number',\n",
        "        'article_link',\n",
        "        'article_views',\n",
        "        'article_downloads',\n",
        "        'journal_title',\n",
        "        'journal_link',\n",
        "        'key_words',\n",
        "        'abstract',\n",
        "        'article_text'\n",
        "      ])\n",
        "      is_column_name = False\n",
        "    for article in articles:\n",
        "      writer.writerow([\n",
        "        article.article_number,\n",
        "        article.article_link,\n",
        "        article.article_views,\n",
        "        article.article_downloads,\n",
        "        article.journal_title,\n",
        "        article.journal_link,\n",
        "        article.key_words,\n",
        "        article.abstract,\n",
        "        article.article_text\n",
        "      ])\n",
        "\n",
        "  # Clear buffer to minimize stoping the parser work\n",
        "  articles = []\n",
        "\n",
        "  # Set time interval between user sessions\n",
        "  time_interval = random.randint(3, 5) # TODO: Change it before running the parser\n",
        "  time.sleep(time_interval)\n",
        "  print('Now, the following proxy has used:', ip_addresses[counter % proxies_quantity].split('@')[1])\n",
        "  print('Sleep finished in', time_interval, 's.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
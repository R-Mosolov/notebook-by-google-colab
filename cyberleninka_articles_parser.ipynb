{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cyberleninka-articles-parser.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "11XQ-TSM-LqQdSc-hwYyEDjXF2edkvbnb",
      "authorship_tag": "ABX9TyP5+O74U2lTpCHeYd3MdBJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-Mosolov/notebook-by-google-colab/blob/main/cyberleninka_articles_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ML_VNngLwIp"
      },
      "source": [
        "!pip uninstall pandas_profiling\n",
        "!pip install pandas_profiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVS9Uj92DcvN"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import requests\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from pandas_profiling import ProfileReport"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS5ShRVuD1OX"
      },
      "source": [
        "fp = urllib.request.urlopen(\"https://cyberleninka.ru/article/c/sociology\")\n",
        "mybytes = fp.read()\n",
        "\n",
        "html_doc = mybytes.decode(\"utf8\")\n",
        "fp.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGnx9cMOEEmk"
      },
      "source": [
        "'''\n",
        "Create data structure\n",
        "'''\n",
        "class Article:\n",
        "  def __init__(self, date, author):\n",
        "    self.date = date\n",
        "    self.author = author"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmY_EbAEE0Ft"
      },
      "source": [
        "'''\n",
        "Add date and an author(-s) of a publication\n",
        "'''\n",
        "articles = []\n",
        "unhandled_data = []\n",
        "\n",
        "for fio in soup.find('ul', attrs={ 'class': 'list' }).find_all('li'):\n",
        "  unhandled_data.append(fio.span.get_text().split(' / '))\n",
        "\n",
        "handled_data = []\n",
        "for data in unhandled_data:\n",
        "  date = data[0]\n",
        "  if len(data) == 2 and len(date) == 4 and data[1] != '':\n",
        "    author = data[1] \n",
        "    articles.append(Article(date, author))\n",
        "\n",
        "for article in articles:\n",
        "  print(article.date)\n",
        "  print(article.author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXxMXwqjAss"
      },
      "source": [
        "'''\n",
        "Save data to a CSV file\n",
        "'''\n",
        "with open('cyberleninka-sociology-articles.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=' ',\n",
        "                            quotechar=';', quoting=csv.QUOTE_MINIMAL)\n",
        "    spamwriter.writerow(['Publication Date', 'Author'])\n",
        "    for article in articles:\n",
        "      spamwriter.writerow([article.date, article.author])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXk2n9L5qhfZ"
      },
      "source": [
        "'''\n",
        "The combinated algorithm to parse authors and publication dates\n",
        "'''\n",
        "class Article:\n",
        "  def __init__(self, date, author):\n",
        "    self.date = date\n",
        "    self.author = author\n",
        "\n",
        "with open('cyberleninka-sociology-articles.csv', 'w', newline='') as csvfile:\n",
        "  writer = csv.writer(csvfile, delimiter=' ',\n",
        "                          quotechar=';', quoting=csv.QUOTE_MINIMAL)\n",
        "  writer.writerow(['Publication Date', 'Author'])\n",
        "\n",
        "for path_number in range(2, 2492):\n",
        "  # Step 1\n",
        "  url = \"https://cyberleninka.ru/article/c/sociology/\" + str(path_number)\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "  html_doc = response.content\n",
        "\n",
        "  # Step 2\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "  # Step 3\n",
        "  articles = []\n",
        "  unhandled_data = []\n",
        "\n",
        "  for fio in soup.find('ul', attrs={ 'class': 'list' }).find_all('li'):\n",
        "    unhandled_data.append(fio.span.get_text().split(' / '))\n",
        "\n",
        "  handled_data = []\n",
        "  for data in unhandled_data:\n",
        "    date = data[0]\n",
        "    if len(data) == 2 and len(date) == 4 and data[1] != '':\n",
        "      author = data[1] \n",
        "      articles.append(Article(date, author))\n",
        "\n",
        "      # Step 4\n",
        "      with open('cyberleninka-sociology-articles__correct-delimiter.csv', 'a') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter=' ',\n",
        "                                quotechar=',', quoting=csv.QUOTE_MINIMAL)\n",
        "        for article in articles:\n",
        "          writer.writerow([article.date, article.author])\n",
        "\n",
        "  for article in articles:\n",
        "    print(article.date)\n",
        "    print(article.author)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV5nuYf9kN15"
      },
      "source": [
        "'''\n",
        "Analize the dataset by using Pandas Profiling\n",
        "'''\n",
        "df = pd.read_csv('/content/drive/MyDrive/Science/Datasets/cyberleninka-sociology-articles/cyberleninka-sociology-articles_1-3__by-S-Yu-Sidorov.csv')\n",
        "\n",
        "profile = ProfileReport(df, title='CyberLeninka Sociology Articles', explorative=True)\n",
        "\n",
        "profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7cIEOqIcsM2",
        "outputId": "9177d387-e7da-47f9-df9b-76816a8157b7"
      },
      "source": [
        "'''\n",
        "The research target #1:\n",
        "Average quantity of authors in common publications\n",
        "'''\n",
        "authors = df['Author']\n",
        "authors\n",
        "\n",
        "ordinary_publications = 0\n",
        "common_publications = 0\n",
        "\n",
        "two_co_authors = 0\n",
        "three_co_authors = 0\n",
        "four_co_authors = 0\n",
        "five_co_authors = 0\n",
        "more_than_five_co_authors = 0\n",
        "\n",
        "for author in authors:\n",
        "  if len(author.split(', ')) > 1:\n",
        "    common_publications += 1\n",
        "\n",
        "    # Calculate co-authors quantity\n",
        "    if len(author.split(', ')) == 2:\n",
        "      two_co_authors += 1\n",
        "    if len(author.split(', ')) == 3:\n",
        "      three_co_authors += 1\n",
        "    if len(author.split(', ')) == 4:\n",
        "      four_co_authors += 1\n",
        "    if len(author.split(', ')) == 5:\n",
        "      five_co_authors += 1\n",
        "    if len(author.split(', ')) > 5:\n",
        "      more_than_five_co_authors += 1\n",
        "  else:\n",
        "    ordinary_publications += 1\n",
        "\n",
        "print('--- Статистика по публикациям ---')\n",
        "print('Всего публикаций:', ordinary_publications + common_publications, 'шт.')\n",
        "print('Одиночных публикаций:', ordinary_publications, 'шт.')\n",
        "print('Коллективных публикаций:', common_publications, 'шт.')\n",
        "\n",
        "print('--- Статистика по соавторам ---')\n",
        "print('Публикаций с 2 соавторами:', two_co_authors, 'шт.')\n",
        "print('Публикаций с 3 соавторами:', three_co_authors, 'шт.')\n",
        "print('Публикаций с 4 соавторами:', four_co_authors, 'шт.')\n",
        "print('Публикаций с 5 соавторами:', five_co_authors, 'шт.')\n",
        "print('Публикаций с более, чем 5 соавторами:', more_than_five_co_authors, 'шт.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Статистика по публикациям ---\n",
            "Всего публикаций: 506459 шт.\n",
            "Одиночных публикаций: 355787 шт.\n",
            "Коллективных публикаций: 150672 шт.\n",
            "--- Статистика по соавторам ---\n",
            "Публикаций с 2 соавторами: 108260 шт.\n",
            "Публикаций с 3 соавторами: 31528 шт.\n",
            "Публикаций с 4 соавторами: 7597 шт.\n",
            "Публикаций с 5 соавторами: 2033 шт.\n",
            "Публикаций с более, чем 5 соавторами: 1254 шт.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAAz9rs7Da9y"
      },
      "source": [
        "'''\n",
        "Import proxies to rotate the parser sessions\n",
        "'''\n",
        "proxies = pd.read_excel('/content/drive/MyDrive/Science/Datasets/proxies/proxies.xlsx')\n",
        "\n",
        "# Example to get a proxy: print(proxies['proxy_with_port'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buU01GFIWQdm",
        "outputId": "5aa50582-3b24-4862-c207-17fc025752b7"
      },
      "source": [
        "'''\n",
        "The template of time interval for delay before requests\n",
        "'''\n",
        "import random\n",
        "import time\n",
        "\n",
        "for i in range(5):\n",
        "  time_interval = random.randint(1, 5)\n",
        "  print('Current time interval:', time_interval, 's.')\n",
        "  time.sleep(time_interval)\n",
        "  print('Sleep finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current time interval: 1 s.\n",
            "Sleep finished\n",
            "Current time interval: 3 s.\n",
            "Sleep finished\n",
            "Current time interval: 4 s.\n",
            "Sleep finished\n",
            "Current time interval: 1 s.\n",
            "Sleep finished\n",
            "Current time interval: 1 s.\n",
            "Sleep finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JWRqKI9D5Oo"
      },
      "source": [
        "'''\n",
        "Show HTML structure of pubclications list to find needed tags\n",
        "'''\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "print(soup.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMMw3SQZBM-"
      },
      "source": [
        "class Article:\n",
        "  def __init__(self, title, date, author, level, license):\n",
        "    self.title = title\n",
        "    self.date = date\n",
        "    self.author = author\n",
        "    self.level = level\n",
        "    self.license = license\n",
        "\n",
        "articles_html = soup.find_all('li')\n",
        "articles_meta = []\n",
        "articles_html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpi2FeScyA5"
      },
      "source": [
        "'''\n",
        "Show paths to get exetended indicators\n",
        "'''\n",
        "for article in articles_html:\n",
        "\n",
        "  # Show titles\n",
        "  print(article.find('div', { 'class': 'title' }).get_text())\n",
        "\n",
        "  # Show dates\n",
        "  print(article.span.get_text().split(' / ')[0])\n",
        "\n",
        "  # Show authors\n",
        "  print(article.span.get_text().split(' / ')[1])\n",
        "\n",
        "  # Show levels\n",
        "  print(article.find('div', { 'class': 'vak' }))\n",
        "  print(article.find('div', { 'class': 'scopus' }))\n",
        "  print(article.find('div', { 'class': 'rsci' }))\n",
        "  print(article.find('div', { 'class': 'esci' }))\n",
        "\n",
        "  # Show licenses\n",
        "  print(article.find('div', { 'class': 'label-cc' }))\n",
        "\n",
        "  print('--- Конец информации о статье ---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJHIiC-0Jb1e"
      },
      "source": [
        "'''\n",
        "The combinated algorithm to parse authors and publication dates\n",
        "with realization of sessions rotation \n",
        "'''\n",
        "class Article:\n",
        "  def __init__(self, date, author):\n",
        "    self.date = date\n",
        "    self.author = author\n",
        "\n",
        "# Initialize proxies for parsing\n",
        "proxies = pd.read_excel('/content/drive/MyDrive/Science/Datasets/proxies/proxies.xlsx')\n",
        "\n",
        "proxies_with_ports = [\n",
        "  proxies['proxy_with_port'][0],\n",
        "  proxies['proxy_with_port'][1],\n",
        "  proxies['proxy_with_port'][2]\n",
        "]\n",
        "proxy_login = proxies['login'][0]\n",
        "proxy_password = proxies['password'][0]\n",
        "\n",
        "ip_addresses = [\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[0],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[1],\n",
        "  'http://' + proxy_login + ':' + proxy_password + '@' + proxies_with_ports[2]\n",
        "]\n",
        "\n",
        "for path_number in range(2, 2492):\n",
        "\n",
        "  # Create an user's session\n",
        "  url = \"https://cyberleninka.ru/article/c/sociology/\" + str(path_number)\n",
        "\n",
        "  user_session = requests.Session()\n",
        "  proxies = {\n",
        "    'http': ip_addresses[path_number % 3],\n",
        "    'https': ip_addresses[path_number % 3]\n",
        "  }\n",
        "\n",
        "  response = user_session.get(url, proxies=proxies)\n",
        "  html_doc = response.content\n",
        "\n",
        "  # Get HTML structure of an page with articles list\n",
        "  soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "  # Get needed data\n",
        "  articles = []\n",
        "  unhandled_data = []\n",
        "\n",
        "  for fio in soup.find('ul', attrs={ 'class': 'list' }).find_all('li'):\n",
        "    unhandled_data.append(fio.span.get_text().split(' / '))\n",
        "\n",
        "  handled_data = []\n",
        "  for data in unhandled_data:\n",
        "    date = data[0]\n",
        "    if len(data) == 2 and len(date) == 4 and data[1] != '':\n",
        "      author = data[1] \n",
        "      articles.append(Article(date, author))\n",
        "\n",
        "      # Save parsed data in an Excel file\n",
        "      with open('cyberleninka-sociology-articles__sessions-rotation.csv', 'w', newline = '') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['article_publication_date', 'article_author'])\n",
        "        for article in articles:\n",
        "          writer.writerow([article.date, article.author])\n",
        "\n",
        "  # Show a log of parser process\n",
        "  for article in articles:\n",
        "    print(article.date)\n",
        "    print(article.author)\n",
        "  \n",
        "  # Set time interval between user sessions\n",
        "  time_interval = random.randint(30, 60)\n",
        "  time.sleep(time_interval)\n",
        "  print(\n",
        "    '--- Sleep finished in', time_interval, '.',\n",
        "    'The current proxy:', ip_addresses[path_number % 3], '---'\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}